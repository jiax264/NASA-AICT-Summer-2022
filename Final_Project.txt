Report on Automated Accessibility Checkers
_______________________________________________________________________________________________________________________________

Rationale

  * Accessibility checkers will catch and miss certain things
 
  * Important to know/measure how "useful" accessibility checkers are
_______________________________________________________________________________________________________________________________

Objectives

  * Produce report for NASA to potentially replace SortSite
  
  * Research and assess several WACs
  
  * Share general information on WACs, manual website testing, and web accessbility
  
  * WAC must be software and spiderable
  
_______________________________________________________________________________________________________________________________

Deliverables (will be accessible)

  * Written report in Word or PDF 
  
  OR
  
  * PowerPoint

_________________________________________________________________________________________________________________________________________________________________

Resources Required 

  WACs (CALL ESD ONCE THE FINAL WACs ARE CHOSEN)
  
    * Tenon (demo)
    
    * AccessibilityOz (demo)
    
    * SiteImprove (demo)
    
    * SilkTide (demo)
    
      - Tests mobile apps and PDFs
      
      - Cloud-based (FEDRAMP?)
      
      - Charges by page number + features wanted + additional services
      
      - Clearly states what it can/can't catch
      
      - Has task management and dashboard system (like WordPress)
      
      - Monitoring is main use, but can set up accounts to be used "ad hoc" (auditing)
      
        --> Monitors every 5 days, but interval COULD be changes
    
    * SortSite (already have)
    
    * DynoMapper (free trial)
    
    * pa11y (free trial)
    
    * Koa11y (free trial)
    
      - Downloaded onto computer (Koa11y.exe file). Paste URL into pop-up window and get your results
      
      - Output is a long HTML file of errors, warnings, and notices
      
      - Catches HTML issues and indicates where in the HTML the issue is
      
      - Only caught one error, two notices on the NASA home page :/
  
   * AMP (demo) (contacted)

     - Responded with Elev11n Community, a page-by-page cloud-based checker (NOT what we want)

       --> Reach out about software-based checker OR move on

   * Axe Monitor (demo) (contacted)

   * PopeTech/WAVE (demo) (contacted)

   * Little Forest (demo) (contacted)

     - Automated testing tool

     - Cloud service, doesn't have FEDRAMP certification (is cloud secure?)

       --> Will take time, but we can probably still get a demo

     - Checks WCAG 2.1

     - "Domain Discovery" = finds websites and generates reports

     - GET THEM: sites (PRIMARY) and requirements (SECONDARY)

   * Odellus (demo) (contacted)
    
 
 Websites
 
    * NASA home page --> https://nasa.gov
    
    * NASA STEM Engagement --> https://www.nasa.gov/stem
    
    * NASA Benefits to You --> https://www.nasa.gov/topics/benefits/index.html
    
    * NASA Live --> https://www.nasa.gov/nasalive
    
    * NASA Missions --> https://www.nasa.gov/missions
    
    * NASA About NASA --> https://www.nasa.gov/about/index.html
    
    * Section 508 home page --> https://www.section508.gov/
    
    * Section 508 Policy and Management --> https://www.section508.gov/manage/
    
    * Section 508 Acquisition --> https://www.section508.gov/buy-sell/
    
    * Section 508 Content Creation --> https://www.section508.gov/create/
    
    * Section 508 Design and Develop --> https://www.section508.gov/develop/
    
    * Section 508 Testing --> https://www.section508.gov/test/
    
    * Section 508 Training, Tools, and Events --> https://www.section508.gov/training/
   
    * ICT Baseline home page --> https://ictbaseline.access-board.gov/
    
    * SEWP home page --> https://www.sewp.nasa.gov/
    
    * SEWP Info Center --> https://www.sewp.nasa.gov/info_center.shtml
   
_______________________________________________________________________________________________________________________________

WAC Requirements

1) Is product easy to use? Does it have a simple user interface?

2) Is it software-based/downloadable on a desktop (preferred)? If not (cloud-based), has it gone through the FEDRAMP security process?

3) Does the software provide guidance on solving issues? Is it designed to accompany (as opposed to replace) manual testing?

4) Is the software and/or user interface 508 compliant?

5) Does the software base its checks on Section 508/WCAG 2.0?

6) What makes your product stand out? What's your favorite feature of the product?

7) What does your product charge for? What services are charged?
_______________________________________________________________________________________________________________________________

WAC Criteria

1) Ease of Setup

2) Ease of Use

3) Reputable Sources

4) Accuracy of Feedback

5) Clarity of Feedback

6) Cost

7) Strengths and Limitations

8) Testing by need (auditing, constant monitoring not too necessary)

  - NASA may not have the resources/manpower to constantly review reports

_______________________________________________________________________________________________________________________________

Timeline

* (Owen) = Owen does it

* (Rio) = Rio does it

* (Owen or Rio) = one of us

* (Owen and Rio) = do individually, then email correspondence

* (Owen and Rio split) = do individually, split work in half (easy stuff so no extensive communication afterwards) 

* (Owen and Rio, but separately) = do individually, then meet in Teams to discuss findings

* (Owen and Rio together) = do work together while meeting in Teams
  
Timeline - 5 weeks total

  Background Research/Acquisition (6/27-7/1) (CALL ESD ABOUT SOFTWARE)
  
    (6/27)
    
      * determine which companies to reach out to / which checkers to investigate (Owen and Rio) 
        - SortSite (yes!), WAVE, Accessibility Insights, SiteImprove, Deque Axe, etc?
          
        - Consider only "spidering" WACs, as opposed to "page-by-page" WACs
          
              --> WAVE, Accessibility Insights = page-by-page
    
              --> Spidering tools = SortSite, SiteImprove
              
              --> Selenium/Axe = open-source spidering tool
              
    (6/28)
              
      * Request demos from vendors (Owen and Rio split) [DONE!]
    
        - Rio: Tenon, AccessibilityOz, SiteImprove, SilkTide
        
        - Owen: AMP, Axe Monitor, PopeTech/WAVE, Little Forest, Odellus
    
      * Download and play with SortSite, pa11y, and koa11y (Owen and Rio) [DONE!]
      

    
    (6/29)
    
      * Finalize websites and requirements (LITERALLY, what do we want?)
      
      * Send websites and requirements to Little Forest and other vendors (Owen and Rio split)
      
      * Read existing consumer reports & get familiarized w format (Owen and Rio split)
        - https://www.nasa.gov/sites/default/files/atoms/files/aoa_neo_report_final_11082018_0.pdf 
        
        - https://www.nasa.gov/sites/default/files/atoms/files/22_20220420_nasa-symposiumnnsa_pae_aoa_methodology_final.pdf 
        
        - https://ntrs.nasa.gov/api/citations/20205008713/downloads/Acoustics_AoA_TWG_VanZante_Final.pdf 
        
      * Schedule follow-up meeting with Little Forest (let's get that demo!) (Rio)
      
      * Respond to PopeTech (Get your stuff together!) (Owen)
            
    (6/30) 
    
      * Perform manual testing on NASA, SEWP, Section 508 to have standard to compare against (Owen & Rio together) 
   
        - schedule meeting(s) w/ each other   
        
     * Send websites and requirements to Little Forest and other vendors (Owen and Rio split)
    
    
  Evaluation Setup - 2 weeks (7/5 - 7/8, 7/11 - 7/15) (CALL ESD ABOUT SOFTWARE)
  
  (7/5 - 7/8)
  
    * Depending on when get access to checkers, get familiarized w/ them as they come in (Owen and Rio, but separately)
    
    * Main task this week is to determine what the report will assess (Owen and Rio)
        - This will be informed by what functionality we observe in the different checkers 
        - Key thing is to not feel boxed in by the categories we set at this point, but do brainstorm what 
        factors/features would be impt to look at 

    * Potential research question(s)
   
      - "How helpful is the checker towards manual testing?"
     
      - "How do these checkers work? Are there certain scenarios where 1 > other?"
         
    * Set up performance evaluation / Create outline for final report
         
       - Quantitative/qualitative way to present findings
 ___________________________________________________________________________________________________________________________
 
  Performance Evaluation - (7/18 - 7/22)
  
   * Use checkers on websites (a lot of this work of getting acquainted with the checkers and their functionality 
   should have already been accomplished in the weeks previously so this should not take super long)
   
   * Collect data & observations 
 ___________________________________________________________________________________________________________________________
   
  Analysis and Conclusion - (7/25 - 7/29 and 8/1 - 8/5)
  
  (7/25 - 7/29) 
  
    * Organize & Analyze data
    
    * Write final report
    
       --> Word, PDF, Slidedoc
       
  (8/1 - 8/5)
  
      * Present!
_______________________________________________________________________________________________________________________________
